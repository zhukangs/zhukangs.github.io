<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Promise</title>
    <link>https://hugo.zkilm.cn/tags/python/</link>
    <description>Recent content in Python on Promise</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-hans</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Wed, 01 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://hugo.zkilm.cn/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>百万级数据导入 Elasticsearch 😌</title>
      <link>https://hugo.zkilm.cn/posts/20210901-python-es/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://hugo.zkilm.cn/posts/20210901-python-es/</guid>
      <description>涉及知识点  Python 读取CSV文件 Elasticsearch 导入json文件  本文目标  完成超过100M的SCV文件上传到ES  导语 如何将100w条记录同步到Es？
Kibana 有个 Data Visualizer 可以直接上传 CSV 文件，但是有个限制是文件不能大于100M。
那么对于大于100M的 CSV 文件我们该如何进行上传到ES呢？
我这里采用的解决方案是通过Es的 Bulk API 进行导入。
参考地址：https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html
但是 Bulk API 对上传的JSON文件大小也会有限制，限制为50M，所以我们会对JSON文件进行拆分，保证每个单JSON文件不超过50M。
整体包括两个步骤：
步骤一：对总CSV文件进行json格式化，格式化为指定的json格式，需要符合Bulk API的上传要求 步骤二：对json文件进行拆分存储，保证单json文件大小不超过50M Py脚本 Python 代码如下(test.py)：
import csv from collections import defaultdict, OrderedDict import json import time import argparse import os parser = argparse.ArgumentParser(description=&amp;#39;自定义CLI传参&amp;#39;) parser.add_argument(&amp;#39;--csv_filename&amp;#39;, &amp;#39;-cf&amp;#39;, help=&amp;#39;csv_filename 选取指定CSV文件名，必要参数&amp;#39;, required=True) parser.add_argument(&amp;#39;--json_dirname&amp;#39;, &amp;#39;-jd&amp;#39;, help=&amp;#39;json_dirname 将作为JSON文件目录名，必要参数&amp;#39;, required=True) args = parser.parse_args() start_time=time.</description>
    </item>
    
  </channel>
</rss>
